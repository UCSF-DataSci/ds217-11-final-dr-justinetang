{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d863b07e",
   "metadata": {},
   "source": [
    "# Q2: Data Cleaning\n",
    "\n",
    "**Phase 3:** Data Cleaning & Preprocessing  \n",
    "**Points: 9 points**\n",
    "\n",
    "**Focus:** Handle missing data, outliers, validate data types, remove duplicates.\n",
    "\n",
    "**Lecture Reference:** See **Lecture 11, Notebook 1** (`11/demo/01_setup_exploration_cleaning.ipynb`), Phase 3 for examples of systematic data cleaning workflows, missing data handling strategies, and outlier detection methods.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38eda39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load data from Q1 (or directly from source)\n",
    "df = pd.read_csv('data/beach_sensors.csv')\n",
    "# If you saved cleaned data from Q1, you can load it:\n",
    "# df = pd.read_csv('output/q1_exploration.csv')  # This won't work - load original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b46d84ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c0/4jn3wz693dbg7vh1dl8yg06r0000gp/T/ipykernel_41885/989812686.py:26: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[non_numeric_cols] = df[non_numeric_cols].fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\n",
    "    'data/beach_sensors.csv',\n",
    "    parse_dates=[\"Measurement Timestamp\"],\n",
    "    index_col=\"Measurement Timestamp\"\n",
    ")\n",
    "\n",
    "# Remove duplicates\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Validate numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "df[numeric_cols] = df[numeric_cols].astype(float)\n",
    "\n",
    "# Handle missing data\n",
    "df[numeric_cols] = df[numeric_cols].ffill()\n",
    "non_numeric_cols = df.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "df[non_numeric_cols] = df[non_numeric_cols].fillna(method='bfill')\n",
    "\n",
    "# Handle outliers by capping at 1st and 99th percentiles\n",
    "for col in numeric_cols:\n",
    "    lower = df[col].quantile(0.01)\n",
    "    upper = df[col].quantile(0.99)\n",
    "    df[col] = df[col].clip(lower, upper)\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv('output/q2_cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e4da34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jutang/Desktop/ds217-11-final-dr-justinetang/output/q2_cleaned_data.csv\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = 'output/q2_cleaned_data.csv'\n",
    "print(os.path.abspath(file_path))\n",
    "print(os.path.exists(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef2e9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Clean the dataset by handling missing data, outliers, validating data types, and removing duplicates.\n",
    "\n",
    "**Time Series Note:** For time series data, forward-fill (`ffill()`) is often appropriate for missing values since sensor readings are continuous. However, you may choose other strategies based on your analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Required Artifacts\n",
    "\n",
    "You must create exactly these 3 files in the `output/` directory:\n",
    "\n",
    "### 1. `output/q2_cleaned_data.csv`\n",
    "**Format:** CSV file\n",
    "**Content:** Cleaned dataset with same structure as original (same columns)\n",
    "**Requirements:**\n",
    "- Same columns as original dataset\n",
    "- Missing values handled (filled, dropped, or imputed)\n",
    "- Outliers handled (removed, capped, or transformed)\n",
    "- Data types validated and converted\n",
    "- Duplicates removed\n",
    "- **No index column** (save with `index=False`)\n",
    "\n",
    "### 2. `output/q2_cleaning_report.txt`\n",
    "**Format:** Plain text file\n",
    "**Content:** Detailed report of cleaning operations\n",
    "**Required information:**\n",
    "- Rows before cleaning: [number]\n",
    "- Missing data handling method: [description]\n",
    "  - Which columns had missing data\n",
    "  - Method used (drop, forward-fill, impute, etc.)\n",
    "  - Number of values handled\n",
    "- Outlier handling: [description]\n",
    "  - Detection method (IQR, z-scores, domain knowledge)\n",
    "  - Which columns had outliers\n",
    "  - Method used (remove, cap, transform)\n",
    "  - Number of outliers handled\n",
    "- Duplicates removed: [number]\n",
    "- Data type conversions: [list any conversions]\n",
    "- Rows after cleaning: [number]\n",
    "\n",
    "**Example format:**\n",
    "```\n",
    "DATA CLEANING REPORT\n",
    "====================\n",
    "\n",
    "Rows before cleaning: 50000\n",
    "\n",
    "Missing Data Handling:\n",
    "- Water Temperature: 2500 missing values (5.0%)\n",
    "  Method: Forward-fill (time series appropriate)\n",
    "  Result: All missing values filled\n",
    "  \n",
    "- Air Temperature: 1500 missing values (3.0%)\n",
    "  Method: Forward-fill, then median imputation for remaining\n",
    "  Result: All missing values filled\n",
    "\n",
    "Outlier Handling:\n",
    "- Water Temperature: Detected 500 outliers using IQR method (3×IQR)\n",
    "  Method: Capped at bounds [Q1 - 3×IQR, Q3 + 3×IQR]\n",
    "  Bounds: [-5.2, 35.8]\n",
    "  Result: 500 values capped\n",
    "\n",
    "Duplicates Removed: 0\n",
    "\n",
    "Data Type Conversions:\n",
    "- Measurement Timestamp: Converted to datetime64[ns]\n",
    "\n",
    "Rows after cleaning: 50000\n",
    "```\n",
    "\n",
    "### 3. `output/q2_rows_cleaned.txt`\n",
    "**Format:** Plain text file\n",
    "**Content:** Single integer number (total rows after cleaning)\n",
    "**Requirements:**\n",
    "- Only the number, no text, no labels\n",
    "- No whitespace before or after\n",
    "- Example: `50000`\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements Checklist\n",
    "\n",
    "- [ ] Missing data handling strategy chosen and implemented\n",
    "- [ ] Outliers detected and handled (IQR method, z-scores, or domain knowledge)\n",
    "- [ ] Data types validated and converted\n",
    "- [ ] Duplicates identified and removed\n",
    "- [ ] Cleaning decisions documented in report\n",
    "- [ ] All 3 required artifacts saved with exact filenames\n",
    "\n",
    "---\n",
    "\n",
    "## Your Approach\n",
    "\n",
    "1. **Handle missing data:**\n",
    "   - Count missing values: `df.isnull().sum()`\n",
    "   - Choose strategy: drop, forward-fill, impute, etc.\n",
    "   - For time series: consider `df.ffill()` (forward-fill is appropriate for continuous sensor readings)\n",
    "   - Implement strategy\n",
    "\n",
    "2. **Detect and handle outliers:**\n",
    "   - Use IQR method: `Q1 = df[col].quantile(0.25)`, `Q3 = df[col].quantile(0.75)`, `IQR = Q3 - Q1`\n",
    "   - Or use z-scores: `z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())`\n",
    "   - Decide: remove, cap, or transform\n",
    "   - Document your reasoning\n",
    "\n",
    "3. **Validate data types:**\n",
    "   - Check data types: `df.dtypes`\n",
    "   - Convert as needed: `pd.to_datetime()`, `pd.to_numeric()`\n",
    "   - Ensure numeric columns are numeric, datetime columns are datetime\n",
    "\n",
    "4. **Remove duplicates:**\n",
    "   - Check: `df.duplicated().sum()`\n",
    "   - Remove: `df.drop_duplicates()`\n",
    "\n",
    "5. **Document and save:**\n",
    "   - Write cleaning report to `output/q2_cleaning_report.txt`\n",
    "   - Save cleaned data to `output/q2_cleaned_data.csv`\n",
    "   - Save row count to `output/q2_rows_cleaned.txt`\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Points\n",
    "\n",
    "- **Missing data:** Should you drop rows, impute values, or forward-fill? Consider: How much data is missing? Is it random or systematic? For time series, forward-fill is often appropriate.\n",
    "- **Outliers:** Are they errors or valid extreme values? Use IQR method or z-scores to detect, then decide: remove, cap, or transform. Document your reasoning.\n",
    "- **Data types:** Are numeric columns actually numeric? Are datetime columns properly formatted? Convert as needed.\n",
    "\n",
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "After Q2, you should have:\n",
    "- [ ] Missing data handled\n",
    "- [ ] Outliers addressed\n",
    "- [ ] Data types validated\n",
    "- [ ] Duplicates removed\n",
    "- [ ] All 3 artifacts saved: `q2_cleaned_data.csv`, `q2_cleaning_report.txt`, `q2_rows_cleaned.txt`\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Continue to `q3_data_wrangling.md` for Data Wrangling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85a6cbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to output/q2_cleaned_data.csv\n",
      "Cleaning report saved to output/q2_cleaning_report.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# Load original dataset\n",
    "df = pd.read_csv(\"data/beach_sensors.csv\")\n",
    "\n",
    "# Copy to cleaned dataset\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Identify numeric and non-numeric columns\n",
    "numeric_cols = df_cleaned.select_dtypes(include=['int64', 'float64', 'int32']).columns\n",
    "non_numeric_cols = df_cleaned.select_dtypes(exclude=['int64', 'float64', 'int32']).columns\n",
    "\n",
    "# Start report\n",
    "report_lines = []\n",
    "report_lines.append(\"DATA CLEANING REPORT\")\n",
    "report_lines.append(\"====================\\n\")\n",
    "\n",
    "# Rows before cleaning\n",
    "rows_before = len(df_cleaned)\n",
    "report_lines.append(f\"Rows before cleaning: {rows_before}\\n\")\n",
    "\n",
    "# Handle Missing Data\n",
    "report_lines.append(\"Missing Data Handling:\")\n",
    "\n",
    "for col in df_cleaned.columns:\n",
    "    missing_count = df_cleaned[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        percent = missing_count / len(df_cleaned) * 100\n",
    "        # Choose strategy\n",
    "        if col in numeric_cols:\n",
    "            strategy = \"Forward-fill (time series appropriate)\"\n",
    "            df_cleaned[col] = df_cleaned[col].ffill()  # implement strategy\n",
    "        else:\n",
    "            strategy = \"Backward-fill for non-numeric columns\"\n",
    "            df_cleaned[col] = df_cleaned[col].bfill()  # implement strategy\n",
    "        report_lines.append(f\"- {col}: {missing_count} missing values ({percent:.1f}%)\")\n",
    "        report_lines.append(f\"  Method: {strategy}\")\n",
    "        report_lines.append(f\"  Result: All missing values filled\")\n",
    "\n",
    "# Detect and Handle Outliers\n",
    "report_lines.append(\"\\nOutlier Handling:\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    # IQR method\n",
    "    Q1 = df_cleaned[col].quantile(0.25)\n",
    "    Q3 = df_cleaned[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3*IQR\n",
    "    upper_bound = Q3 + 3*IQR\n",
    "    # Count outliers\n",
    "    outliers_lower = (df_cleaned[col] < lower_bound).sum()\n",
    "    outliers_upper = (df_cleaned[col] > upper_bound).sum()\n",
    "    total_outliers = outliers_lower + outliers_upper\n",
    "    if total_outliers > 0:\n",
    "        # Handle outliers by capping\n",
    "        df_cleaned[col] = df_cleaned[col].clip(lower_bound, upper_bound)\n",
    "        report_lines.append(f\"- {col}: Detected {total_outliers} outliers using IQR method (3×IQR)\")\n",
    "        report_lines.append(f\"  Method: Capped at bounds [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        report_lines.append(f\"  Result: {total_outliers} values capped\")\n",
    "\n",
    "# Validate Data Types\n",
    "report_lines.append(\"\\nData Type Conversions:\")\n",
    "\n",
    "# Timestamps\n",
    "if 'Measurement Timestamp' in df_cleaned.columns:\n",
    "    df_cleaned['Measurement Timestamp'] = pd.to_datetime(df_cleaned['Measurement Timestamp'])\n",
    "    report_lines.append(\"- Measurement Timestamp: Converted to datetime64[ns]\")\n",
    "elif 'Measurement Timestamp Label' in df_cleaned.columns:\n",
    "    df_cleaned['Measurement Timestamp Label'] = pd.to_datetime(df_cleaned['Measurement Timestamp Label'])\n",
    "    report_lines.append(\"- Measurement Timestamp Label: Converted to datetime64[ns]\")\n",
    "\n",
    "# Numeric columns\n",
    "for col in numeric_cols:\n",
    "    df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
    "    report_lines.append(f\"- {col}: Converted to float\")\n",
    "\n",
    "\n",
    "# Remove Duplicates\n",
    "duplicates_removed = df_cleaned.duplicated().sum()\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "report_lines.append(f\"\\nDuplicates Removed: {duplicates_removed}\")\n",
    "\n",
    "# Rows after cleaning\n",
    "rows_after = len(df_cleaned)\n",
    "report_lines.append(f\"\\nRows after cleaning: {rows_after}\")\n",
    "\n",
    "\n",
    "# Save cleaned dataset and report\n",
    "df_cleaned.to_csv(\"output/q2_cleaned_data.csv\", index=False)\n",
    "\n",
    "report_path = \"output/q2_cleaning_report.txt\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "print(f\"Cleaned dataset saved to output/q2_cleaned_data.csv\")\n",
    "print(f\"Cleaning report saved to {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abfa6e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning (195873) saved to output/q2_rows_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# Load cleaned data\n",
    "df_cleaned = pd.read_csv(\"output/q2_cleaned_data.csv\")\n",
    "\n",
    "# Get number of rows\n",
    "rows_after_cleaning = len(df_cleaned)\n",
    "\n",
    "# Save to file\n",
    "with open(\"output/q2_rows_cleaned.txt\", \"w\") as f:\n",
    "    f.write(str(rows_after_cleaning))\n",
    "\n",
    "print(f\"Rows after cleaning ({rows_after_cleaning}) saved to output/q2_rows_cleaned.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
